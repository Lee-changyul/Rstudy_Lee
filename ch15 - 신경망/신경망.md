신경망(ANN, Artificial Neural Network) : 분류와 예측, 군집이 모두 가능

- 로지스틱 회귀분석, 다중회귀분석을 여러개 뭉쳐 놓은 것 
- 인간 두뇌구조와 유사한 지도학습 방법
- 뇌신경망의 원리를 이용
- 학습을 통하여 데이터들간의 패턴 혹은 관계를 습득
- 활용분야 : 파산예측, 신용카드 사기 검출, 고객 관계 관리, 금융계 시계열 예측, 의학적 상태 진단, 화상 인식, 생산공정 제어, 자동주행  

신경망 연결(ppt 11)
- 기본 논리  : 정보입력 -> 가중치 부여(시냅스) -> 수집된 정보를 합산 -> 정보를 특정 함수를 사용하여 계산 -> 값을 출력
- 하나의 뉴런 = 퍼셉트론(perceptron) 

신경망의 주요 층(퍼셉트론의 기본 구조)

- 입력층(input layer)  -> 은닉층(hidden layer) -> 출력층(output layer)
- 입력층 : 각 입력변수에 대응되는 마디들로 구성
- 은닉층
* 입력층으로부터 전달되는 변수값들의 선형결합(linear combination)을 비선형함수(nonlinear function)로 처리하여 출력층 또는 
     다른 은닉층에 전달
- 출력층 : 목표변수(target)에 대응하는 마디

정보처리 과정 : 함수를 이용 (2가지 함수) - 출력함수

- 결합함수(Combination Function) : 입력값들을 결합 (회귀분석의 논리) - input * weight 가중치를 찾는 것이 가장 중요 
- 활성함수(Activation Function) : 입력값의 결합 -> 출력값으로 변환(로지스틱 함수의 기본 논리) - 값을 다 더해서 확률로 변경
  * 입력변수 또는 은닉마디의 결합을 변환하는 함수
  * 신경망은 다양한 출력함수를 사용 : Sigmoid(일반적으로 사용, 기본 모델), 
    Leaky ReLU, tanh, Maxout, ReLU(성능이 좋아 사용 빈도 증가), ELU  

신경망의 정보처리 

입력층
- 변수 유형   
   * 수치형 :  출력함수의 유형에 따라 표준화를 거쳐서 0-1 사이 값 또는 -1 ~ 1 사이의 값을 가지도록 함. 함수마다 다름 
     ** sigmoid 함수(로지스틱 함수)는 0 ~ 1 사이의 값을 내보내기 때문에 입력값도 그 기준에 맞춤
     ** tanh 함수  : -1 ~ 1사이 값으로 변환
   * 범주형 변수 : 0 ~ 1 사이의 값(0, 0.5, 1) / 가변수(dummy)로 전환 : 1,0,0  0,1,0 0,0,1 => 주로 가변수 많이 사용 
   * 비대칭 분포 :  Log  변환 등 (소득 분포 등 특정한 데이터)
 - 가중치 : 0 ~ 1 사이 값을 가짐 
 - 계산 : 행렬식에 의하여 한번에 계산함(가중치는 행, 입력값은 열) => 이를 다시 함수를 통해 하나의 값으로 출력

출력층  
- 분류(범주형 변수) : Class = m 일 경우, 노드수 = m 또는 m-1
- 예측(수치형 변수) : 1개  


신경망의 종류(ppt 17)

- 단층퍼셉트론 : 로지스틱 회귀함수의 기본 논리 - 입력 => 출력 (히든층이 없음)
- 다층퍼셉트론 : 은닉층을 두는 것 (신경망) - 은닉층의 노드를 여러개 두면 사회과학(예측과 분류)에서는 충분히 활용가능 
  * 은닉층의 노드 수 : 많을 수록 정확도가 올라가지만 과적합의 문제 발생
- 딥러닝 : 은닉층을 여러 겹 두는 것 - 빅데이터는 여러층 필요 

신경망의 학습

- 최상의 예측결과를 도출하는 연결강도(𝜃, 𝑤)를 찾는 것
   * 연결강도(𝜃, 𝑤)를 구할 때 초기 값은 일반적으로 -0.05 ~ + 0.05로 무작위로 주어짐 
   * 𝜃(bias)는 통계학의 절편, 편차, 오차 - 상수로 기준값이 됨  /  𝑤는 가중치(weight) 
* Generation(epoch): 모든 데이터를 한번 학습하는 것
   * 학습방법
  ** case updating: 1개 데이터(case)를 한 후에 바로 에러를 이용하여 연결강도(𝜃, 𝑤)를 수정 - 시간이 많이 걸림
  ** Batch updating: 모든 데이터를 1 epoch 한 후에 통합된 에러를 이용하여 연결강도(𝜃, 𝑤)를 수정 - 1 epoch 1 updating

- Back Propagation (역전파 알고리즘”)
* 학습: 최상의 예측결과를 도출하는 연결강도(𝜃, 𝑤)를 찾는 것
* 방법: 비용함수(SSE: Sum of Squared Error)를 이용하여 에러를 최소화 하도록 연결강도 조정
     ** 편차(예측값과 실제값의 오차)를 줄이는 것 , error 를 신경망에서는 비용(cost)이라고 부름
* 역전파 알고리즘: 출력층의 에러를 이용하여 역으로 연결강도 조정
   ** 경사하강법(Gradient descent) : 미분을 이용하여 기울기가 최소인 지점 찾기
      ** 경사하강법에 따른 고려사항
      1) 모멘텀(Momentum) 0 ~ 2 사이 값 : 어느 정도 기존의 방향을 유지할 것인지 조정. 방향을 변경할지 조절
            *** 변화가 이루어질 때 어느정도 올라가도 멈추지 말고 계속 계산해 보라는 의미 
      2) 학습률(Learning Rate) 0 ~ 1 사이 값 : 새로운 정보를 얼마나 반영할지를 조절. 변화량 조절
            *** 작게 주면 학습에 시간이 많이 걸리고 너무 크게 주면 수치가 크게 변화(반대로 튕김)  => 방법과 정해진 기준은 없다.
 
장단점 

- 좋은 예측성능 but 해석이 어려움(Black Box Model)
- 잡음이 많은 데이터에 좋은 성능 but 학습을 위해서 많은 양의 데이터 필요 
- 변수들 사이의 복잡한 관계 잘 파악 but 중요한 변수 선택하지 못함 
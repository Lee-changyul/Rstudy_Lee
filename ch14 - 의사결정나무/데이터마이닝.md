1. 데이터 마이닝 분석절차 

 - 1. 데이터 마이닝 프로젝트 목적을 명확히 정의
 - 2. 분석에 필요한 데이터 획득
 - 3. 데이터의 탐색, 정제, 전처리
 - 4. 데이터 축소(필요시)
 - 5. 데이터 마이닝 작업 결정(분류, 예측, 군집 등)
 - 6. 데이터 분할(지도학습의 경우) : 훈련(training, 모델 개발), 검증(test, 모델 검증), 평가용
 - 7. 분석기법 선택 : 회귀분석, CART, 인공신경망 등
 - 8. 알고리즘을 사용하여 과제 수행
 - 9. 알고리즘 결과 해석
 - 10. 모델 적용

2. 데이터 분석 사전단계 : 필요에 따라 활용 

 - 샘플링 : 모델 구축시 DB에 있는 모든 데이터를 사용하지 않음 
   - 희소사건에 대한 오버샘플링 주의
    * 관심있는 데이터가 희귀할 경우 : 사기카드 거래(정상에 비해 데이터가 적음)
    * 방법 1: 샘플링시에 희소사건에 가중치 주어 해결
    * 방법 2: 오분류시에 가중치 주어 해결(방법2 선호 - 모델 평가시에)

 - 데이터의 탐색, 정제, 전처리
   * 범주형 변수 : 가변수로 처리 
   * 이상치 제거 : 측정오류나 잘못된 입력
   * 결측치(missing data)
     ** 데이터가 많을 경우 : 삭제
     ** 데이터가 적을 경우 : 중앙값으로 대체 

 - 데이터의 정규화 (표준화)
   * 군집분석의 경우 정규화 필요 
   * 거리 척도의 경우
   * 단위에 따라 영향을 받을 경우 : 값이 큰 것에 영향을 미침 - 표준화 처리 (평균 0, 편차 1)

3. 예측력과 과적합  

 - 과적합 : 학습용 데이터에 적합, 학습용 집합에서 잡음도도 모형화하기 때문에 평가용 집합에서 전체 오차는 일반적으로 증가
   * 기준 : 검증용 데이터에 적합도를 높여라 

===========

1. 의사결정나무와 불순도 검증법 

1. CART(Clasification And Regression Tree)
 - 불순도 알고리즘 : Gini 지수(Gini Index) - 불확실성을 의미함. 낮을 수록 좋음 
 - 학습데이터를 통해 나무를 성장시키고, 검증용 데이터를 통해 가지치기
 - 2번 복원 추출했을 때 나올 수 있는 확률 
 - 최대 G(A) =0.5(두 집단이 동일할 때)
   * 그룹의 class 가 동일하면 낮아지고, 그룹의 class가 다양하면 높아짐
 - 예제 : 충성고객과 탈퇴고객을 분류하는 규칙 생성 (ppt 15)

2. C4.5, C5.0
 - 불순도 알고리즘 : 엔트로피(Entropy), Information gain,  이득율(gain ratio)
 - 가지치기 : 학습데이터만 사용하여 나무를 성장 및 가지치기
 - 정보이론(엔트로피) : 컴퓨터의 정보량을 계산하는 이론, 컴퓨터는 0과1의 이진수   

   * log2로 계산하는 이유 - bit 수로 정보계산 
   * 앞에서 -log2로 계산 : 단순하게 정보량을 양수로 보기 위해서 앞에 -를 붙여줌 (2번 계산하는 공식으로 인해서 이렇게 전환)

 - IG : 정보이익 = 불확실성이 감소한 정보의 양이므로 클수록 좋음 (CART와 반대)
  * IG = E(before) - E(after) : E는 엔트로피 => 사전의 불확실성 - 사후의 불확실성 
  * 엔트로피와 지니지수는 같은 개념임 : 개념 차제는 불확실성을 의미 
  * 지니지수는 불확실성 자체를 가지고 이야기했다면, 정보이익 불확실성이 얼마나 줄었는지에 관심에 있음 : 전후의 차이

 - 엔트로피는 아무것도 구분하지 못하였을 때 1이 나옴. 1부터 시작  (제곱이냐 로그이냐에 따라 기준 값이 달라짐)

 - 이득에서 이득률로의 전환(Information gain ratio)
  * 가지가 많은 경우에 information gain이 높은 경향
  * IV(Intrinsic value) : 정규화 - 가지가 많으면 감점 부여 
  * 이득률 IGR(A) = IG(A)/IV(A) : 원래 있는 것에다가 감점을 넣어 주겠다는 의미 

3. CHAID (캐이드)

 - 불순도 알고리즘 : Chi-square 통계량, F 통계량 이용 - 관측빈도와 기대빈도의 차이를 활용하여 통계량 계산하고 검증 
 - 가지치기 없음 : 나무성장에만 활용하며 Stop 규칙 적용 - 이유 : 카이스퀘어 값 자체가 유의미성을 포함하고 있음
     (기준점이 있음 : 어림잡아 4이상 - 정확한 것은 표를 통해서 확인해야 하며 R이 자동으로 계산해주므로 신경 안써도 됨) 
 - 입력변수와 목표변수가 모두 범주형만 가능 

=============

1. 의사결정나무 생성 과정

 - 나무모델 생성
 - 과적합 문제 해결
 - 검증
 - 해석 및 예측 

 - 중요 : 재귀적 분할에서 범주형 변수일 경우 처리방법 - 계산은 컴퓨터가 알아서 해준다. 
  * 예 : 예측변수가 A, B, C 일 경우 => 이진값으로 분류하여 처리(CART 는 이진 분류)
   ** {A} and {B, C} A와 나머지
      {B} and {A, C} B와 나머지
      {C} and {A, B} C와 나머지 
  * 그런데 예측변수가 4개 이상일 때에는 어떻게 그룹을 나누어서 분할할까? 
    ** CART는 이진분류로 무조건 두개로 분할  
    ** C4.5, CHAID 는 자유롭게 분할(다지분할이 가능하므로) 
  